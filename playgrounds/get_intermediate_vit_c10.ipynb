{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C10でFine-tuningしたViTモデルのフォワードパスを実行する\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, math\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append(\"../src\")\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import DefaultDataCollator, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from utils.helper import get_device\n",
    "from utils.vit_util import processor, transforms, compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dsのsaveとload\n",
    "from datasets import load_from_disk\n",
    "cifar10 = load_from_disk(\"../dataset/c10\")\n",
    "cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# デバイス (cuda, or cpu) の取得\n",
    "device = get_device()\n",
    "# 読み込まれた時にリアルタイムで前処理を適用するようにする\n",
    "cifar10_preprocessed = cifar10.with_transform(transforms)\n",
    "# バッチごとの処理のためのdata_collator\n",
    "data_collator = DefaultDataCollator()\n",
    "# ラベルを示す文字列のlist\n",
    "labels = cifar10_preprocessed[\"train\"].features[\"label\"].names\n",
    "# pretrained modelのロード\n",
    "pretrained_dir = \"/src/src/out_vit_c10\"\n",
    "model = ViTForImageClassification.from_pretrained(pretrained_dir).to(device)\n",
    "model.eval()\n",
    "# 学習時の設定をロード\n",
    "training_args = torch.load(os.path.join(pretrained_dir, \"training_args.bin\"))\n",
    "# Trainerオブジェクトの作成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=cifar10_preprocessed[\"train\"],\n",
    "    eval_dataset=cifar10_preprocessed[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad37e4e154d64d5a80e460bd6d446a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10859/3760055383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry_dic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for entry_dic in tqdm(cifar10_preprocessed[\"train\"].iter(batch_size=32), total=len(cifar10_preprocessed[\"train\"])//32+1):\n",
    "    x, y = entry_dic[\"pixel_values\"].to(device), entry_dic[\"labels\"]\n",
    "    output = model.forward(x, output_hidden_states=True)\n",
    "    print(len(output.hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'hidden_states', 'attentions', 'intermediate_states'])\n",
      "[13, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "mf = model.forward(dum_input, output_hidden_states=True, output_attentions=True, output_intermediate_states=True)\n",
    "print(mf.keys())\n",
    "# 後ろの3つのキーのタプルの長さを取得\n",
    "print([len(mf[k]) for k in list(mf.keys())[-3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "mf.hidden_states[0].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[0][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[1].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[1][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[2].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[2][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[3].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[3][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[4].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[4][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[5].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[5][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[6].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[6][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[7].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[7][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[8].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[8][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[9].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[9][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[10].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[10][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[11].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[11][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[12].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[12][:, 0, :].shape = torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(mf.hidden_states))\n",
    "for i, hs in enumerate(mf.hidden_states):\n",
    "    print(f\"mf.hidden_states[{i}].shape = {hs.shape}\")\n",
    "    print(f\"mf.hidden_states[{i}][:, 0, :].shape = {hs[:, 0, :].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "mf.attentions[0].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[1].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[2].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[3].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[4].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[5].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[6].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[7].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[8].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[9].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[10].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[11].shape = torch.Size([5, 12, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "print(len(mf.attentions))\n",
    "for i, attn in enumerate(mf.attentions):\n",
    "    print(f\"mf.attentions[{i}].shape = {attn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "mf.intermediate_states[0].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[1].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[2].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[3].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[4].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[5].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[6].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[7].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[8].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[9].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[10].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[11].shape = torch.Size([5, 197, 3072])\n"
     ]
    }
   ],
   "source": [
    "print(len(mf.intermediate_states))\n",
    "for i, med in enumerate(mf.intermediate_states):\n",
    "    print(f\"mf.intermediate_states[{i}].shape = {med.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
