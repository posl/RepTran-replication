{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 22:41:25.038648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 22:41:26.119763: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-18 22:41:26.119918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-18 22:41:26.119930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "../src/utils/vit_util.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  met_acc = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "C10„ÅßFine-tuning„Åó„ÅüViT„É¢„Éá„É´„ÅÆ„Éï„Ç©„ÉØ„Éº„Éâ„Éë„Çπ„ÇíÂÆüË°å„Åô„Çã\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, math\n",
    "sys.path.append(\"../src\")\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import DefaultDataCollator, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from utils.helper import get_device\n",
    "from utils.vit_util import processor, transforms, compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/plain_text-d4c080360fb556b0/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536cee5ecc694c479dad4efd53db9acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# „Éá„Éê„Ç§„Çπ (cuda, or cpu) „ÅÆÂèñÂæó\n",
    "device = get_device()\n",
    "# dataset„Çí„É≠„Éº„Éâ (ÂàùÂõû„ÅÆË™≠„ÅøËæº„Åø„Å†„Åë„ÇÑ„ÇÑÊôÇÈñì„Åã„Åã„Çã)\n",
    "cifar10 = load_dataset(\"cifar10\")\n",
    "# Ë™≠„ÅøËæº„Åæ„Çå„ÅüÊôÇ„Å´„É™„Ç¢„É´„Çø„Ç§„É†„ÅßÂâçÂá¶ÁêÜ„ÇíÈÅ©Áî®„Åô„Çã„Çà„ÅÜ„Å´„Åô„Çã\n",
    "cifar10_preprocessed = cifar10.with_transform(transforms)\n",
    "# „Éê„ÉÉ„ÉÅ„Åî„Å®„ÅÆÂá¶ÁêÜ„ÅÆ„Åü„ÇÅ„ÅÆdata_collator\n",
    "data_collator = DefaultDataCollator()\n",
    "# „É©„Éô„É´„ÇíÁ§∫„ÅôÊñáÂ≠óÂàó„ÅÆlist\n",
    "labels = cifar10_preprocessed[\"train\"].features[\"label\"].names\n",
    "# pretrained model„ÅÆ„É≠„Éº„Éâ\n",
    "pretrained_dir = \"/src/src/out_vit_c10\"\n",
    "model = ViTForImageClassification.from_pretrained(pretrained_dir).to(device)\n",
    "model.eval()\n",
    "# Â≠¶ÁøíÊôÇ„ÅÆË®≠ÂÆö„Çí„É≠„Éº„Éâ\n",
    "training_args = torch.load(os.path.join(pretrained_dir, \"training_args.bin\"))\n",
    "# Trainer„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆ‰ΩúÊàê\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=cifar10_preprocessed[\"train\"],\n",
    "    eval_dataset=cifar10_preprocessed[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "vit.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "li = 11\n",
    "key = f\"vit.encoder.layer.{li}.intermediate.dense\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(key):\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_preprocessed[\"train\"][:5][\"pixel_values\"].to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'hidden_states', 'attentions', 'intermediate_states'])\n",
      "[13, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "mf = model.forward(cifar10_preprocessed[\"train\"][:5][\"pixel_values\"].to(device), output_hidden_states=True, output_attentions=True, output_intermediate_states=True)\n",
    "print(mf.keys())\n",
    "# Âæå„Çç„ÅÆ3„Å§„ÅÆ„Ç≠„Éº„ÅÆ„Çø„Éó„É´„ÅÆÈï∑„Åï„ÇíÂèñÂæó\n",
    "print([len(mf[k]) for k in list(mf.keys())[-3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "mf.hidden_states[0].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[0][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[1].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[1][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[2].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[2][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[3].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[3][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[4].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[4][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[5].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[5][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[6].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[6][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[7].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[7][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[8].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[8][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[9].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[9][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[10].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[10][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[11].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[11][:, 0, :].shape = torch.Size([5, 768])\n",
      "mf.hidden_states[12].shape = torch.Size([5, 197, 768])\n",
      "mf.hidden_states[12][:, 0, :].shape = torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(mf.hidden_states))\n",
    "for i, hs in enumerate(mf.hidden_states):\n",
    "    print(f\"mf.hidden_states[{i}].shape = {hs.shape}\")\n",
    "    print(f\"mf.hidden_states[{i}][:, 0, :].shape = {hs[:, 0, :].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "mf.attentions[0].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[1].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[2].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[3].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[4].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[5].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[6].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[7].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[8].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[9].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[10].shape = torch.Size([5, 12, 197, 197])\n",
      "mf.attentions[11].shape = torch.Size([5, 12, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "print(len(mf.attentions))\n",
    "for i, attn in enumerate(mf.attentions):\n",
    "    print(f\"mf.attentions[{i}].shape = {attn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "mf.intermediate_states[0].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[1].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[2].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[3].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[4].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[5].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[6].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[7].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[8].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[9].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[10].shape = torch.Size([5, 197, 3072])\n",
      "mf.intermediate_states[11].shape = torch.Size([5, 197, 3072])\n"
     ]
    }
   ],
   "source": [
    "print(len(mf.intermediate_states))\n",
    "for i, med in enumerate(mf.intermediate_states):\n",
    "    print(f\"mf.intermediate_states[{i}].shape = {med.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
