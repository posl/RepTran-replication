{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "330915dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd41ddc",
   "metadata": {},
   "source": [
    "# 中間状態の更新処理をLoRAとして実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.sys.path.append(\"../src\")\n",
    "from utils.constant import ViTExperiment\n",
    "from transformers import ViTForImageClassification\n",
    "from utils.helper import get_device\n",
    "device = get_device()\n",
    "\n",
    "pretrained_dir = getattr(ViTExperiment, \"c100\").OUTPUT_DIR.format(k=0)\n",
    "model = ViTForImageClassification.from_pretrained(pretrained_dir).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9de3bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (repair): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (repair): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.vit.encoder.layer[-2].intermediate)\n",
    "print(model.vit.encoder.layer[-1].intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9420b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'encoder.layer.3.intermediate.repair.weight', 'encoder.layer.9.intermediate.repair.weight', 'encoder.layer.2.intermediate.repair.weight', 'encoder.layer.0.intermediate.repair.weight', 'encoder.layer.6.intermediate.repair.weight', 'encoder.layer.11.intermediate.repair.weight', 'encoder.layer.4.intermediate.repair.weight', 'encoder.layer.5.intermediate.repair.weight', 'classifier.weight', 'encoder.layer.7.intermediate.repair.weight', 'encoder.layer.1.intermediate.repair.weight', 'encoder.layer.10.intermediate.repair.weight', 'encoder.layer.8.intermediate.repair.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6144 || all params: 199052546 || trainable%: 0.003086622162572088\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"vit.encoder.layer.11.intermediate.repair\"],  # 明示的に11層だけ指定\n",
    ")\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model.eval()\n",
    "lora_model = get_peft_model(model, lora_cfg)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3affd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (repair): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (repair): Linear(\n",
      "    in_features=3072, out_features=3072, bias=False\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model.base_model.model.vit.encoder.layer[-2].intermediate)\n",
    "print(lora_model.base_model.model.vit.encoder.layer[-1].intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11bfdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vit.encoder.layer.11.intermediate.repair.lora_A.default.weight  (1, 3072)\n",
      "base_model.model.vit.encoder.layer.11.intermediate.repair.lora_B.default.weight  (3072, 1)\n"
     ]
    }
   ],
   "source": [
    "# どのパラメータが “学習対象” になっているか確認\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:60s}  {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17af48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vit.encoder.layer.11.intermediate.repair → LoRA injected\n"
     ]
    }
   ],
   "source": [
    "# どのレイヤにloraが挿入されたか確認\n",
    "for name, module in lora_model.named_modules():\n",
    "    if hasattr(module, \"lora_A\"):\n",
    "        print(name, \"→ LoRA injected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd99c8e6",
   "metadata": {},
   "source": [
    "# ViTモジュール変更後の学習が問題ないかチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "77c147b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DefaultDataCollator, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from utils.vit_util import processor, transforms, transforms_c100, compute_metrics\n",
    "import torch\n",
    "\n",
    "dataset_dir = ViTExperiment.DATASET_DIR\n",
    "ds = load_from_disk(os.path.join(dataset_dir, f\"c100_fold0\"))\n",
    "tf_func = transforms_c100\n",
    "label_col = \"fine_label\"\n",
    "\n",
    "# 読み込まれた時にリアルタイムで前処理を適用するようにする\n",
    "ds_preprocessed = ds.with_transform(tf_func)\n",
    "# バッチごとの処理のためのdata_collator\n",
    "data_collator = DefaultDataCollator()\n",
    "# ラベルを示す文字列のlist\n",
    "labels = ds_preprocessed[\"train\"].features[label_col].names\n",
    "\n",
    "# 学習の設定\n",
    "batch_size = ViTExperiment.BATCH_SIZE\n",
    "logging_steps = len(ds_preprocessed[\"train\"]) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3428208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Key: vit.encoder.layer.0.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-0.0068, -0.0041, -0.0011,  ..., -0.0170, -0.0345, -0.0077],\n",
      "        [ 0.0241, -0.0017,  0.0212,  ...,  0.0049,  0.0206,  0.0086],\n",
      "        [-0.0070, -0.0263, -0.0012,  ..., -0.0171,  0.0389,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0103, -0.0158,  ...,  0.0230,  0.0158, -0.0026],\n",
      "        [-0.0284, -0.0115,  0.0029,  ..., -0.0030, -0.0244,  0.0234],\n",
      "        [-0.0018,  0.0036,  0.0327,  ...,  0.0135,  0.0163, -0.0211]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.1.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[ 0.0172, -0.0247,  0.0552,  ..., -0.0100,  0.0045,  0.0093],\n",
      "        [-0.0122, -0.0097,  0.0232,  ..., -0.0339,  0.0100,  0.0094],\n",
      "        [-0.0391,  0.0108, -0.0155,  ..., -0.0002, -0.0028, -0.0198],\n",
      "        ...,\n",
      "        [ 0.0111, -0.0093,  0.0042,  ..., -0.0140, -0.0157, -0.0186],\n",
      "        [ 0.0057, -0.0280, -0.0284,  ..., -0.0009, -0.0098, -0.0103],\n",
      "        [ 0.0008,  0.0171, -0.0020,  ..., -0.0341,  0.0386,  0.0420]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.2.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-0.0136,  0.0451, -0.0010,  ...,  0.0113,  0.0209, -0.0184],\n",
      "        [ 0.0061, -0.0119, -0.0048,  ..., -0.0105,  0.0048,  0.0198],\n",
      "        [ 0.0236, -0.0276,  0.0129,  ..., -0.0070, -0.0180, -0.0034],\n",
      "        ...,\n",
      "        [-0.0076,  0.0323,  0.0012,  ...,  0.0016,  0.0224, -0.0193],\n",
      "        [-0.0153,  0.0225,  0.0190,  ...,  0.0142, -0.0054,  0.0148],\n",
      "        [-0.0158,  0.0011,  0.0377,  ...,  0.0028,  0.0082, -0.0321]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.3.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-0.0040,  0.0421, -0.0232,  ..., -0.0691,  0.0451,  0.0107],\n",
      "        [-0.0059, -0.0316, -0.0038,  ..., -0.0159,  0.0307,  0.0084],\n",
      "        [-0.0146, -0.0025, -0.0036,  ...,  0.0018,  0.0002, -0.0115],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0120,  0.0055,  ..., -0.0083,  0.0297,  0.0111],\n",
      "        [ 0.0241, -0.0055,  0.0304,  ...,  0.0056,  0.0089,  0.0023],\n",
      "        [-0.0038, -0.0229, -0.0122,  ...,  0.0069, -0.0112, -0.0078]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.4.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-2.4472e-02,  1.9284e-02, -2.1072e-02,  ...,  2.4978e-03,\n",
      "         -9.8608e-08, -1.1729e-02],\n",
      "        [ 4.4739e-02, -7.2234e-03,  1.3695e-03,  ..., -1.9264e-02,\n",
      "         -4.6301e-02, -1.6538e-02],\n",
      "        [ 2.0840e-02, -3.2127e-03, -1.7431e-02,  ..., -3.7302e-02,\n",
      "         -2.3832e-02, -2.4735e-02],\n",
      "        ...,\n",
      "        [-1.9438e-02, -1.5030e-02,  4.9476e-02,  ...,  3.7165e-04,\n",
      "          1.9069e-03,  4.1460e-03],\n",
      "        [ 3.7963e-02,  1.0310e-02, -1.4544e-02,  ..., -1.7006e-02,\n",
      "         -1.5047e-02,  1.5230e-02],\n",
      "        [ 3.2770e-03,  1.4965e-02, -8.3083e-03,  ...,  1.5051e-02,\n",
      "         -3.3895e-04, -2.7335e-02]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.5.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[ 0.0257, -0.0073,  0.0273,  ...,  0.0177, -0.0227, -0.0133],\n",
      "        [-0.0018,  0.0130, -0.0303,  ..., -0.0164,  0.0320,  0.0093],\n",
      "        [ 0.0209,  0.0370,  0.0214,  ..., -0.0164, -0.0028, -0.0028],\n",
      "        ...,\n",
      "        [-0.0089, -0.0311, -0.0122,  ...,  0.0289, -0.0599,  0.0089],\n",
      "        [-0.0257,  0.0281,  0.0308,  ...,  0.0152,  0.0019,  0.0256],\n",
      "        [-0.0224,  0.0272,  0.0157,  ..., -0.0054,  0.0190, -0.0126]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.6.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-0.0071, -0.0012,  0.0132,  ..., -0.0129, -0.0122,  0.0029],\n",
      "        [-0.0092,  0.0261, -0.0118,  ..., -0.0079, -0.0048,  0.0260],\n",
      "        [ 0.0160, -0.0028, -0.0103,  ..., -0.0170, -0.0274,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0260,  0.0041,  ...,  0.0066, -0.0280,  0.0065],\n",
      "        [ 0.0016, -0.0085,  0.0224,  ..., -0.0012, -0.0078,  0.0037],\n",
      "        [ 0.0533,  0.0013, -0.0248,  ...,  0.0305,  0.0391,  0.0193]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.7.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-3.0169e-02,  5.5155e-03,  1.4457e-03,  ...,  2.1370e-02,\n",
      "          2.5443e-02,  3.0853e-03],\n",
      "        [ 1.0785e-02, -1.2451e-03,  7.0420e-03,  ..., -2.1807e-02,\n",
      "          1.1390e-02, -4.3577e-02],\n",
      "        [ 1.9654e-02,  2.3737e-02,  4.7368e-03,  ...,  6.1944e-05,\n",
      "         -3.4377e-02,  1.5208e-02],\n",
      "        ...,\n",
      "        [-2.6789e-02,  2.1569e-02,  1.4217e-02,  ..., -2.6011e-02,\n",
      "          1.3601e-02,  1.0194e-02],\n",
      "        [-4.0041e-02,  6.9534e-03, -1.0903e-02,  ..., -8.8488e-03,\n",
      "          4.1349e-02,  2.1622e-02],\n",
      "        [-5.3682e-03, -2.0632e-02, -7.6717e-03,  ...,  2.3511e-02,\n",
      "         -7.3626e-03,  5.3385e-03]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.8.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-2.0029e-03, -1.6813e-02, -2.1918e-02,  ..., -2.0186e-02,\n",
      "         -2.1015e-02, -8.0716e-03],\n",
      "        [ 2.9329e-05,  3.8977e-02, -5.9195e-03,  ..., -1.4195e-03,\n",
      "          2.6865e-03, -3.6219e-02],\n",
      "        [ 2.5380e-02,  2.5228e-02,  1.0006e-02,  ..., -1.7377e-02,\n",
      "         -3.3823e-03,  2.1123e-03],\n",
      "        ...,\n",
      "        [ 6.1996e-04, -1.6699e-02, -2.6951e-02,  ...,  1.3040e-03,\n",
      "          5.4627e-03,  4.3413e-02],\n",
      "        [-1.0050e-04,  3.4897e-02, -1.8588e-02,  ...,  1.1391e-03,\n",
      "         -5.9134e-03,  1.3899e-02],\n",
      "        [ 1.3538e-02, -2.8733e-03, -1.6325e-02,  ...,  1.5850e-03,\n",
      "         -6.7175e-03, -6.3490e-03]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.9.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[ 0.0153,  0.0047,  0.0113,  ..., -0.0175,  0.0037, -0.0154],\n",
      "        [-0.0325, -0.0164,  0.0110,  ...,  0.0194,  0.0089,  0.0094],\n",
      "        [-0.0127,  0.0154,  0.0229,  ...,  0.0043, -0.0069, -0.0343],\n",
      "        ...,\n",
      "        [-0.0030, -0.0543,  0.0494,  ..., -0.0418, -0.0372,  0.0027],\n",
      "        [-0.0036,  0.0075, -0.0188,  ..., -0.0298,  0.0077, -0.0027],\n",
      "        [ 0.0057, -0.0041,  0.0180,  ..., -0.0327, -0.0145, -0.0144]],\n",
      "       device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.10.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-2.1813e-02, -2.8795e-02, -2.6436e-03,  ..., -6.5951e-03,\n",
      "         -2.9889e-04,  4.0867e-02],\n",
      "        [-7.6186e-03, -1.4144e-03, -1.2556e-02,  ..., -8.9845e-03,\n",
      "          2.4847e-02,  2.8531e-02],\n",
      "        [ 4.1108e-03, -3.5258e-03, -1.4586e-03,  ...,  1.1597e-02,\n",
      "         -2.5140e-02, -1.7905e-02],\n",
      "        ...,\n",
      "        [-4.4094e-02,  9.8251e-03,  5.1379e-02,  ...,  3.9968e-02,\n",
      "         -1.0669e-02,  3.4283e-03],\n",
      "        [-1.5107e-02, -6.1134e-04,  8.2651e-03,  ..., -1.5212e-02,\n",
      "          6.0136e-03,  1.3721e-02],\n",
      "        [-1.4961e-02, -3.0529e-03,  3.4063e-03,  ..., -2.3927e-02,\n",
      "         -5.3308e-03,  4.7413e-05]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.11.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[-0.0193, -0.0033, -0.0127,  ..., -0.0080, -0.0143, -0.0070],\n",
      "        [-0.0127,  0.0121, -0.0141,  ...,  0.0240, -0.0023, -0.0021],\n",
      "        [ 0.0061, -0.0237,  0.0119,  ...,  0.0141,  0.0238, -0.0288],\n",
      "        ...,\n",
      "        [ 0.0087, -0.0361,  0.0294,  ..., -0.0159,  0.0039, -0.0201],\n",
      "        [ 0.0208, -0.0087,  0.0023,  ...,  0.0099, -0.0162, -0.0053],\n",
      "        [ 0.0018,  0.0138, -0.0179,  ...,  0.0302,  0.0404, -0.0367]],\n",
      "       device='cuda:0')\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# pretrained modelのロード\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    ViTExperiment.ViT_PATH,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ").to(device)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# 'repair' という名前を含むパラメータを抽出して表示\n",
    "repair_keys = [k for k in state_dict.keys() if 'repair' in k]\n",
    "\n",
    "# 'repair' を含むキーを抽出\n",
    "for key in state_dict:\n",
    "    if 'repair' in key:\n",
    "        print(f\"🔑 Key: {key}\")\n",
    "        print(f\"🧠 Weight Tensor (shape={state_dict[key].shape}):\")\n",
    "        print(state_dict[key])\n",
    "        print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5b282b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Key: vit.encoder.layer.0.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.1.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.2.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.3.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.4.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.5.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.6.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.7.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.8.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.9.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.10.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n",
      "🔑 Key: vit.encoder.layer.11.intermediate.repair.weight\n",
      "🧠 Weight Tensor (shape=torch.Size([3072, 3072])):\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# pretrained modelのロード\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    ViTExperiment.ViT_PATH,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ").to(device)\n",
    "\n",
    "# intermediate.repairレイヤの恒等初期化を明示的にやり直す (訓練した時はrepairなかったので)\n",
    "# repairレイヤを増やす前のモデルからfrom_pretrainedする時はこれが必要？\n",
    "with torch.no_grad():\n",
    "    for i in range(len(model.vit.encoder.layer)):\n",
    "        model.vit.encoder.layer[i].intermediate.repair.weight.copy_(torch.eye(3072))\n",
    "        model.vit.encoder.layer[i].intermediate.repair.weight.requires_grad = False\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# 'repair' という名前を含むパラメータを抽出して表示\n",
    "repair_keys = [k for k in state_dict.keys() if 'repair' in k]\n",
    "\n",
    "# 'repair' を含むキーを抽出\n",
    "for key in state_dict:\n",
    "    if 'repair' in key:\n",
    "        print(f\"🔑 Key: {key}\")\n",
    "        print(f\"🧠 Weight Tensor (shape={state_dict[key].shape}):\")\n",
    "        print(state_dict[key])\n",
    "        print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6b708482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 43.127157% of total params (85875556 / 199121764)\n"
     ]
    }
   ],
   "source": [
    "use_lora = False\n",
    "\n",
    "if use_lora:\n",
    "    # LoRAの設定\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=1,\n",
    "        lora_alpha=1,\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"vit.encoder.layer.11.intermediate.repair\"],  # 明示的に11層だけ指定\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    model.train()\n",
    "    # model.print_trainable_parameters()  # LoRA param ∼ 100 k だけ\n",
    "else:\n",
    "    # print(model.vit.encoder.layer[-1].intermediate.repair.weight)\n",
    "    model.train()\n",
    "    # print(model.vit.encoder.layer[-1].intermediate.repair.weight)\n",
    "    # 最終レイヤだけ訓練可能にする\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if \"vit.encoder.layer.11\" not in name:\n",
    "    #         param.requires_grad = False\n",
    "\n",
    "# 訓練可能なパラメータ数の割合\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params / total_params:.6%} of total params ({trainable_params} / {total_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7b195331",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False, # img列がないとエラーになるので必要\n",
    "    evaluation_strategy=\"epoch\", # エポックの終わりごとにeval_datasetで評価\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"error\",\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "34137e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_preprocessed[\"train\"]), len(ds_preprocessed[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d340c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.args.label_names=None\n",
      "default_label_names=['labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/transformers-4.30.2/src/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.145500</td>\n",
       "      <td>0</td>\n",
       "      <td>{'accuracy': 0.17}</td>\n",
       "      <td>{'f1': 0.10282909578684225}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.771600</td>\n",
       "      <td>0</td>\n",
       "      <td>{'accuracy': 0.17}</td>\n",
       "      <td>{'f1': 0.09838475296221776}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.404483795166016, logits=tensor([[-0.0537,  0.1130,  0.0404,  ...,  0.1223, -0.1201,  0.0136],\n",
      "        [-0.3235, -0.0561, -0.1776,  ...,  0.2033, -0.2095, -0.0517],\n",
      "        [-0.2215, -0.0197, -0.0164,  ...,  0.2380, -0.2078, -0.1188],\n",
      "        ...,\n",
      "        [ 0.0806, -0.0085, -0.0609,  ...,  0.1577,  0.1127,  0.2258],\n",
      "        [-0.2563,  0.0035, -0.0694,  ...,  0.1316, -0.2163,  0.0128],\n",
      "        [-0.2347, -0.1956, -0.1319,  ...,  0.1039, -0.1405,  0.1511]],\n",
      "       device='cuda:0'), labels=tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,\n",
      "        70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.49449348449707, logits=tensor([[-0.1759,  0.0829, -0.0594,  ...,  0.1920, -0.2288,  0.0972],\n",
      "        [-0.0441, -0.0234,  0.2048,  ...,  0.1287,  0.0015, -0.0070],\n",
      "        [-0.1753,  0.0496,  0.1112,  ...,  0.3630, -0.1223,  0.0627],\n",
      "        ...,\n",
      "        [-0.0799,  0.0552,  0.0341,  ..., -0.1737,  0.0301, -0.1508],\n",
      "        [-0.1876,  0.0299, -0.1033,  ...,  0.3185, -0.1022, -0.0791],\n",
      "        [-0.1341,  0.0171, -0.0414,  ...,  0.1004, -0.0748,  0.1174]],\n",
      "       device='cuda:0'), labels=tensor([41, 93, 56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,\n",
      "         4,  6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.47900390625, logits=tensor([[-0.0212, -0.1090, -0.2055,  ..., -0.0484, -0.0831,  0.0161],\n",
      "        [-0.1117, -0.0248,  0.0597,  ...,  0.0363, -0.1233,  0.0039],\n",
      "        [-0.0962, -0.0640, -0.1204,  ...,  0.2573, -0.1168,  0.0020],\n",
      "        ...,\n",
      "        [ 0.0186, -0.0106, -0.0297,  ...,  0.0379, -0.2542,  0.0217],\n",
      "        [-0.1766, -0.0389, -0.0470,  ...,  0.2756, -0.1929, -0.0376],\n",
      "        [-0.3061,  0.1101,  0.0084,  ...,  0.5963, -0.0827,  0.0249]],\n",
      "       device='cuda:0'), labels=tensor([ 7, 35, 43, 82, 63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78,\n",
      "        54,  6, 29, 89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.406916618347168, logits=tensor([[-0.2218,  0.0679,  0.0705, -0.2667,  0.0881, -0.2412, -0.0702, -0.0563,\n",
      "          0.0905, -0.1872,  0.0427,  0.1006, -0.2040,  0.0515,  0.1146,  0.1222,\n",
      "         -0.0603, -0.0481, -0.0752,  0.0583, -0.2096, -0.3426, -0.0522, -0.2015,\n",
      "          0.0228, -0.1995, -0.2420, -0.0939, -0.0837, -0.0667, -0.0472, -0.1450,\n",
      "         -0.1090, -0.0548, -0.2269,  0.1029,  0.3745, -0.0236,  0.0890, -0.0501,\n",
      "         -0.0455, -0.1793,  0.2946, -0.2349, -0.0693,  0.0060, -0.2078,  0.0442,\n",
      "         -0.0892, -0.0565,  0.3611, -0.0850, -0.0740, -0.0517, -0.1228, -0.0851,\n",
      "          0.2197,  0.0110, -0.0784, -0.0623, -0.1293, -0.0819,  0.0121, -0.1670,\n",
      "          0.0228, -0.0935, -0.0815, -0.0075,  0.0158,  0.0404,  0.1361,  0.0068,\n",
      "         -0.0756,  0.0391, -0.1377, -0.1044, -0.1589, -0.1457, -0.1658, -0.1786,\n",
      "         -0.0959, -0.0342, -0.0443,  0.0217,  0.0633, -0.2613, -0.0310, -0.1267,\n",
      "         -0.1116, -0.0799, -0.0302, -0.1321, -0.2116,  0.1418, -0.1370,  0.0336,\n",
      "         -0.1588,  0.4354, -0.1444, -0.1119],\n",
      "        [-0.0999,  0.0059, -0.0211,  0.0633,  0.0752, -0.1322, -0.1290, -0.0746,\n",
      "         -0.2460, -0.0295, -0.1775,  0.1091, -0.1788,  0.0686, -0.0850,  0.0433,\n",
      "         -0.0249, -0.0799, -0.0690,  0.0581,  0.0254,  0.0383, -0.1352,  0.0775,\n",
      "         -0.0233, -0.0596, -0.0246, -0.0536, -0.1089, -0.1366,  0.0173, -0.2149,\n",
      "          0.0532,  0.2786, -0.1218,  0.0866, -0.1648, -0.1362,  0.1240, -0.0593,\n",
      "         -0.0492, -0.1661,  0.4636, -0.1982, -0.2716, -0.0926,  0.0661,  0.1293,\n",
      "         -0.0843, -0.1217, -0.0762, -0.1801, -0.0082, -0.2312, -0.2681,  0.1026,\n",
      "          0.0798, -0.0247, -0.1968,  0.0768, -0.2608, -0.1345, -0.2181, -0.2131,\n",
      "          0.0539, -0.1245, -0.1784, -0.0903, -0.0407,  0.0715, -0.1429,  0.1369,\n",
      "         -0.1203,  0.1399,  0.0116,  0.2461, -0.1900,  0.0436,  0.0272,  0.0501,\n",
      "         -0.0852, -0.0478,  0.2189,  0.1292,  0.1273, -0.0580, -0.0667,  0.0202,\n",
      "         -0.1402,  0.1933,  0.1611, -0.2603,  0.0560, -0.1552,  0.0151,  0.1260,\n",
      "         -0.0349,  0.2601, -0.0679,  0.0497],\n",
      "        [-0.1304,  0.0776,  0.0005,  0.0124, -0.0181, -0.1054,  0.0552,  0.1050,\n",
      "          0.1289, -0.0734, -0.1251,  0.1379, -0.0787, -0.0804, -0.0915, -0.0401,\n",
      "         -0.1491, -0.0761, -0.2083,  0.0386, -0.0678, -0.1493, -0.0738, -0.0906,\n",
      "         -0.0560, -0.0880, -0.0418,  0.1884, -0.0197,  0.0147, -0.1508, -0.2470,\n",
      "         -0.1143,  0.1267, -0.1969,  0.0105, -0.1033,  0.0537,  0.2963,  0.1547,\n",
      "          0.0899, -0.0458,  0.1139, -0.3250, -0.2038, -0.1624, -0.0133, -0.0609,\n",
      "         -0.0016, -0.0372,  0.0266, -0.0997, -0.0459,  0.0607, -0.2195, -0.2021,\n",
      "          0.1910, -0.4206,  0.0541,  0.4028, -0.0711, -0.0100, -0.1234, -0.0113,\n",
      "         -0.1567, -0.0264,  0.0084, -0.1211,  0.1381,  0.0418, -0.1047,  0.0308,\n",
      "          0.0381, -0.0101, -0.1023, -0.2019, -0.1569, -0.1122,  0.0087, -0.2073,\n",
      "         -0.0666,  0.2135,  0.1473,  0.1638,  0.1487, -0.1795,  0.1176, -0.0530,\n",
      "         -0.0254,  0.0918, -0.0281, -0.0727,  0.0426, -0.2153,  0.0856,  0.0148,\n",
      "         -0.1841,  0.1957, -0.1510,  0.0959],\n",
      "        [-0.2784, -0.0407, -0.0956, -0.1802, -0.0803, -0.3929, -0.0222,  0.0337,\n",
      "          0.3068, -0.0877, -0.0599,  0.4630, -0.1830, -0.1590, -0.1344,  0.0128,\n",
      "         -0.1914,  0.0339, -0.3250, -0.0464,  0.0172, -0.0567, -0.1318, -0.0038,\n",
      "         -0.0194, -0.0960, -0.1776,  0.0573, -0.0591,  0.0179, -0.2745, -0.0792,\n",
      "         -0.3748,  0.0611, -0.1151,  0.0031, -0.0638, -0.0420,  0.4331,  0.0983,\n",
      "         -0.2248, -0.2051,  0.3222, -0.1092, -0.2323, -0.0517,  0.0892,  0.0080,\n",
      "         -0.0890, -0.2895,  0.1683, -0.0202, -0.1456, -0.0632, -0.1660, -0.2092,\n",
      "          0.0916, -0.1655, -0.1286, -0.1473, -0.0377, -0.1274, -0.3037, -0.2782,\n",
      "         -0.1283, -0.1677,  0.0615, -0.1510, -0.1853,  0.2184, -0.1160,  0.1410,\n",
      "         -0.1284, -0.0526, -0.0645, -0.0057, -0.2135, -0.3519, -0.0451, -0.0803,\n",
      "         -0.1098,  0.1728, -0.0741,  0.1163,  0.0221, -0.1785, -0.1072, -0.1003,\n",
      "         -0.1141,  0.2386,  0.1785, -0.3935, -0.1133,  0.0509,  0.1834, -0.0898,\n",
      "         -0.1160,  0.2994, -0.1807, -0.1081]], device='cuda:0'), labels=tensor([50, 75, 37, 35], device='cuda:0')\n",
      "output=EvalLoopOutput(predictions=array([[-0.05371128,  0.11301155,  0.0404494 , ...,  0.12229294,\n",
      "        -0.12006719,  0.01356465],\n",
      "       [-0.32346374, -0.05606814, -0.177611  , ...,  0.20327055,\n",
      "        -0.20950492, -0.0517456 ],\n",
      "       [-0.22149406, -0.01974768, -0.01640813, ...,  0.2380089 ,\n",
      "        -0.2078472 , -0.11882803],\n",
      "       ...,\n",
      "       [-0.09985677,  0.00591852, -0.02114162, ...,  0.2601082 ,\n",
      "        -0.06785453,  0.04974939],\n",
      "       [-0.13037202,  0.07761676,  0.00050569, ...,  0.19569293,\n",
      "        -0.15097341,  0.09591004],\n",
      "       [-0.27838883, -0.04071264, -0.09557658, ...,  0.29938886,\n",
      "        -0.1806659 , -0.10812746]], dtype=float32), label_ids=array([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92,\n",
      "       97, 70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61, 41, 93,\n",
      "       56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,  4,\n",
      "        6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65,  7, 35, 43, 82,\n",
      "       63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78, 54,  6, 29,\n",
      "       89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21, 50, 75, 37, 35]), metrics={'eval_loss': 0, 'eval_accuracy': {'accuracy': 0.17}, 'eval_f1': {'f1': 0.10282909578684225}}, num_samples=100)\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.366483211517334, logits=tensor([[ 1.5699e-04,  2.0267e-01, -6.3821e-02,  ...,  1.4709e-01,\n",
      "         -1.6070e-01,  1.7589e-02],\n",
      "        [-3.6810e-01, -6.6620e-02, -2.1477e-01,  ...,  1.1763e-01,\n",
      "         -2.1690e-01, -4.0902e-02],\n",
      "        [-2.4328e-01, -4.0826e-02, -2.2223e-02,  ...,  2.4390e-01,\n",
      "         -2.0304e-01, -1.5328e-01],\n",
      "        ...,\n",
      "        [-1.6924e-02, -1.5182e-02, -6.6746e-03,  ...,  2.0660e-01,\n",
      "          2.7777e-02,  2.3257e-01],\n",
      "        [-2.4855e-01, -6.0633e-02, -1.6568e-01,  ...,  1.6969e-01,\n",
      "         -2.1329e-01, -5.3479e-02],\n",
      "        [-2.1721e-01, -1.6887e-01, -1.5247e-01,  ...,  5.8937e-02,\n",
      "         -2.1616e-01,  9.7459e-02]], device='cuda:0'), labels=tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,\n",
      "        70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.48283576965332, logits=tensor([[-0.2301,  0.0773, -0.1390,  ...,  0.2349, -0.2568,  0.0721],\n",
      "        [-0.0774, -0.0640,  0.1325,  ...,  0.1567,  0.0141, -0.0097],\n",
      "        [-0.1011, -0.0266, -0.1045,  ...,  0.3281, -0.0755,  0.0635],\n",
      "        ...,\n",
      "        [-0.1209,  0.0286,  0.0136,  ..., -0.1421, -0.0227, -0.0507],\n",
      "        [-0.1755,  0.0177, -0.0736,  ...,  0.4332, -0.1021, -0.1002],\n",
      "        [-0.1963, -0.0017, -0.1111,  ...,  0.1095, -0.0508,  0.1693]],\n",
      "       device='cuda:0'), labels=tensor([41, 93, 56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,\n",
      "         4,  6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.469005107879639, logits=tensor([[-0.0123, -0.0645, -0.2394,  ..., -0.0351, -0.0670, -0.0083],\n",
      "        [-0.1451, -0.0656,  0.0510,  ...,  0.0430, -0.0719, -0.0049],\n",
      "        [-0.0681, -0.0703, -0.1419,  ...,  0.2723, -0.1523, -0.0644],\n",
      "        ...,\n",
      "        [-0.0103, -0.0140, -0.0244,  ...,  0.0567, -0.2794,  0.0188],\n",
      "        [-0.2015, -0.0765, -0.0406,  ...,  0.3018, -0.2283, -0.0360],\n",
      "        [-0.2791,  0.0503, -0.0059,  ...,  0.5878, -0.1312,  0.0346]],\n",
      "       device='cuda:0'), labels=tensor([ 7, 35, 43, 82, 63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78,\n",
      "        54,  6, 29, 89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.372093200683594, logits=tensor([[-0.2515,  0.0463,  0.0263, -0.2568,  0.1239, -0.2474, -0.1095, -0.0562,\n",
      "          0.0832, -0.2117,  0.0376,  0.1142, -0.2487,  0.0690,  0.1466,  0.1050,\n",
      "         -0.0884, -0.0514, -0.0122, -0.0043, -0.2302, -0.3242, -0.0354, -0.1735,\n",
      "         -0.0045, -0.2000, -0.2771, -0.1034, -0.0890, -0.0620, -0.1136, -0.1746,\n",
      "         -0.1601, -0.0862, -0.2167,  0.1277,  0.3445, -0.0202,  0.1254, -0.0072,\n",
      "         -0.0530, -0.1989,  0.2888, -0.2560, -0.1534, -0.0388, -0.2335,  0.0080,\n",
      "         -0.1358, -0.0037,  0.3732, -0.1560, -0.0570, -0.0728, -0.1455, -0.0427,\n",
      "          0.2235, -0.0406, -0.1106, -0.1499, -0.2036, -0.0886,  0.0276, -0.1596,\n",
      "          0.0469, -0.0590, -0.1055,  0.0093,  0.0755,  0.0652,  0.1704, -0.0269,\n",
      "         -0.1018,  0.0497, -0.2131, -0.1050, -0.1461, -0.1626, -0.1478, -0.1980,\n",
      "         -0.0735, -0.0636,  0.0010,  0.0903,  0.0709, -0.2758, -0.0195, -0.1952,\n",
      "         -0.1584, -0.0828, -0.0279, -0.1582, -0.2267,  0.1555, -0.1284,  0.0163,\n",
      "         -0.1517,  0.4741, -0.1390, -0.0662],\n",
      "        [-0.0285,  0.0328, -0.0091,  0.0722,  0.0068, -0.1411, -0.1671, -0.0538,\n",
      "         -0.2310, -0.0087, -0.1607,  0.1726, -0.2180, -0.0043, -0.0892,  0.0574,\n",
      "         -0.0995, -0.0526, -0.0895,  0.0667, -0.0809,  0.0561, -0.1660,  0.0670,\n",
      "         -0.0692, -0.1243, -0.0618, -0.0385, -0.0598, -0.1629, -0.0099, -0.2144,\n",
      "          0.0397,  0.2324, -0.1615,  0.0961, -0.1543, -0.1111,  0.1265, -0.0350,\n",
      "         -0.0933, -0.1293,  0.4864, -0.1780, -0.3306, -0.0224,  0.0037,  0.1364,\n",
      "         -0.1201, -0.0194, -0.0185, -0.1475, -0.0259, -0.2705, -0.2795,  0.0633,\n",
      "          0.0555, -0.0517, -0.2596,  0.0657, -0.3119, -0.2467, -0.2401, -0.2115,\n",
      "          0.0396, -0.1473, -0.2015, -0.1029, -0.0665,  0.0965, -0.1510,  0.0791,\n",
      "         -0.0631,  0.1838, -0.0170,  0.3343, -0.1750,  0.0333,  0.0569, -0.0217,\n",
      "         -0.0619, -0.0649,  0.2104,  0.0966,  0.0701, -0.1155, -0.0837, -0.0183,\n",
      "         -0.1466,  0.1433,  0.1349, -0.2586,  0.0602, -0.1815,  0.0282,  0.1250,\n",
      "         -0.0524,  0.2507, -0.0723,  0.0129],\n",
      "        [-0.0857,  0.0793,  0.0021, -0.0202, -0.0573, -0.1563,  0.1252,  0.0605,\n",
      "          0.1325, -0.0928, -0.1090,  0.2449, -0.1305, -0.1983, -0.0481, -0.0205,\n",
      "         -0.1947, -0.0810, -0.1883,  0.0409, -0.0279, -0.2223, -0.0130,  0.0507,\n",
      "         -0.1433, -0.1244, -0.0797,  0.1668, -0.0437,  0.1117, -0.1153, -0.2541,\n",
      "         -0.1471,  0.2211, -0.2359,  0.0430, -0.0808,  0.0045,  0.3072,  0.1627,\n",
      "          0.0850, -0.0864,  0.1785, -0.4479, -0.3168, -0.2152,  0.0124, -0.0914,\n",
      "         -0.0688, -0.0149,  0.0651, -0.1747, -0.0253, -0.0329, -0.2948, -0.1539,\n",
      "          0.1751, -0.4888, -0.0036,  0.3452, -0.1483, -0.0894, -0.0875, -0.0736,\n",
      "         -0.1158, -0.1911, -0.0226, -0.2338,  0.0608,  0.0933, -0.0907,  0.0069,\n",
      "         -0.0557,  0.0229, -0.0551, -0.2308, -0.3578, -0.1666, -0.0867, -0.2987,\n",
      "         -0.0475,  0.1581,  0.1966,  0.1785,  0.1979, -0.2993,  0.0742, -0.0981,\n",
      "         -0.0319,  0.1954, -0.0413, -0.1832, -0.0779, -0.2569,  0.1022, -0.0458,\n",
      "         -0.3177,  0.3036, -0.2023,  0.0658],\n",
      "        [-0.1517, -0.0822, -0.0465, -0.1734, -0.0638, -0.2651,  0.0209,  0.0751,\n",
      "          0.3971, -0.0883, -0.0312,  0.4323, -0.1894, -0.0994, -0.0625,  0.0038,\n",
      "         -0.1659,  0.1012, -0.2398, -0.0155, -0.0170,  0.0011, -0.1276,  0.0167,\n",
      "         -0.0281, -0.2074, -0.1980,  0.0661, -0.0673, -0.0964, -0.3034, -0.0756,\n",
      "         -0.4146,  0.0707, -0.1080,  0.0642,  0.0128,  0.0166,  0.3192,  0.0455,\n",
      "         -0.2963, -0.2539,  0.3212, -0.1021, -0.2373, -0.0703,  0.0164, -0.0010,\n",
      "         -0.1678, -0.3118,  0.2374, -0.0627, -0.1750, -0.0339, -0.2107, -0.2226,\n",
      "          0.1708, -0.1905, -0.2018, -0.1439, -0.1043, -0.1119, -0.2614, -0.2873,\n",
      "         -0.0497, -0.1907,  0.0566, -0.1196, -0.2165,  0.2420, -0.1611,  0.0287,\n",
      "         -0.1009, -0.0856, -0.1500, -0.0279, -0.2475, -0.3098, -0.0937, -0.1366,\n",
      "         -0.0452,  0.2372, -0.1213,  0.2260,  0.0653, -0.1156, -0.1354, -0.0964,\n",
      "         -0.1760,  0.1710,  0.2246, -0.4258, -0.1568,  0.0817,  0.2956, -0.0567,\n",
      "         -0.1075,  0.3192, -0.1572, -0.0944]], device='cuda:0'), labels=tensor([50, 75, 37, 35], device='cuda:0')\n",
      "output=EvalLoopOutput(predictions=array([[ 1.5698560e-04,  2.0267394e-01, -6.3821100e-02, ...,\n",
      "         1.4708740e-01, -1.6069882e-01,  1.7588554e-02],\n",
      "       [-3.6809570e-01, -6.6620134e-02, -2.1477117e-01, ...,\n",
      "         1.1763298e-01, -2.1690126e-01, -4.0901806e-02],\n",
      "       [-2.4327618e-01, -4.0825903e-02, -2.2222757e-02, ...,\n",
      "         2.4389857e-01, -2.0304182e-01, -1.5328118e-01],\n",
      "       ...,\n",
      "       [-2.8477410e-02,  3.2772031e-02, -9.0638241e-03, ...,\n",
      "         2.5067475e-01, -7.2260305e-02,  1.2859564e-02],\n",
      "       [-8.5729100e-02,  7.9348490e-02,  2.0966688e-03, ...,\n",
      "         3.0356550e-01, -2.0225516e-01,  6.5824367e-02],\n",
      "       [-1.5169533e-01, -8.2152992e-02, -4.6524197e-02, ...,\n",
      "         3.1923589e-01, -1.5724802e-01, -9.4424158e-02]], dtype=float32), label_ids=array([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92,\n",
      "       97, 70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61, 41, 93,\n",
      "       56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,  4,\n",
      "        6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65,  7, 35, 43, 82,\n",
      "       63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78, 54,  6, 29,\n",
      "       89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21, 50, 75, 37, 35]), metrics={'eval_loss': 0, 'eval_accuracy': {'accuracy': 0.17}, 'eval_f1': {'f1': 0.09838475296221776}}, num_samples=100)\n"
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "# 訓練データを100件だけに制限\n",
    "ds_limited = ds_preprocessed.copy()\n",
    "ds_limited[\"train\"] = ds_preprocessed[\"train\"].select(range(100))\n",
    "ds_limited[\"test\"] = ds_preprocessed[\"test\"].select(range(100))\n",
    "# NOTE: 表示されるプログレスバーの分母の数字は，num_epoch*num_sample/batch_size\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds_preprocessed[\"train\"],\n",
    "    eval_dataset=ds_preprocessed[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f02d03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 パラメータ一覧（trainable / untrainable 含む）\n",
      "✅ trainable  | vit.embeddings.cls_token                                     | shape: (1, 1, 768)               | #params: 768\n",
      "✅ trainable  | vit.embeddings.position_embeddings                           | shape: (1, 197, 768)             | #params: 151296\n",
      "✅ trainable  | vit.embeddings.patch_embeddings.projection.weight            | shape: (768, 3, 16, 16)          | #params: 589824\n",
      "✅ trainable  | vit.embeddings.patch_embeddings.projection.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.0.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.0.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.0.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.0.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.0.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.0.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.0.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.0.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.0.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.1.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.1.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.1.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.1.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.1.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.1.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.1.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.1.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.1.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.2.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.2.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.2.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.2.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.2.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.2.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.2.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.2.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.2.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.3.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.3.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.3.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.3.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.3.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.3.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.3.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.3.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.3.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.4.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.4.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.4.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.4.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.4.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.4.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.4.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.4.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.4.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.5.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.5.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.5.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.5.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.5.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.5.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.5.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.5.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.5.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.6.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.6.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.6.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.6.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.6.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.6.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.6.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.6.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.6.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.7.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.7.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.7.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.7.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.7.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.7.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.7.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.7.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.7.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.8.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.8.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.8.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.8.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.8.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.8.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.8.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.8.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.8.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.attention.attention.query.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.9.attention.attention.query.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.attention.attention.key.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.9.attention.attention.key.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.attention.attention.value.weight         | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.9.attention.attention.value.bias           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.attention.output.dense.weight            | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.9.attention.output.dense.bias              | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.intermediate.dense.weight                | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.9.intermediate.dense.bias                  | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.9.intermediate.repair.weight               | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.9.output.dense.weight                      | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.9.output.dense.bias                        | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.layernorm_before.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.layernorm_before.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.layernorm_after.weight                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.9.layernorm_after.bias                     | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.attention.attention.query.weight        | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.10.attention.attention.query.bias          | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.attention.attention.key.weight          | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.10.attention.attention.key.bias            | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.attention.attention.value.weight        | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.10.attention.attention.value.bias          | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.attention.output.dense.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.10.attention.output.dense.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.intermediate.dense.weight               | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.10.intermediate.dense.bias                 | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.10.intermediate.repair.weight              | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.10.output.dense.weight                     | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.10.output.dense.bias                       | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.layernorm_before.weight                 | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.layernorm_before.bias                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.layernorm_after.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.10.layernorm_after.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.attention.attention.query.weight        | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.11.attention.attention.query.bias          | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.attention.attention.key.weight          | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.11.attention.attention.key.bias            | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.attention.attention.value.weight        | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.11.attention.attention.value.bias          | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.attention.output.dense.weight           | shape: (768, 768)                | #params: 589824\n",
      "✅ trainable  | vit.encoder.layer.11.attention.output.dense.bias             | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.intermediate.dense.weight               | shape: (3072, 768)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.11.intermediate.dense.bias                 | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | vit.encoder.layer.11.intermediate.repair.weight              | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | vit.encoder.layer.11.output.dense.weight                     | shape: (768, 3072)               | #params: 2359296\n",
      "✅ trainable  | vit.encoder.layer.11.output.dense.bias                       | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.layernorm_before.weight                 | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.layernorm_before.bias                   | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.layernorm_after.weight                  | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.encoder.layer.11.layernorm_after.bias                    | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.layernorm.weight                                         | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | vit.layernorm.bias                                           | shape: (768,)                    | #params: 768\n",
      "✅ trainable  | classifier.weight                                            | shape: (100, 768)                | #params: 76800\n",
      "✅ trainable  | classifier.bias                                              | shape: (100,)                    | #params: 100\n",
      "\n",
      "📊 Summary\n",
      "Trainable: 85,875,556 / 199,121,764 (43.13%)\n"
     ]
    }
   ],
   "source": [
    "print(\"📋 パラメータ一覧（trainable / untrainable 含む）\")\n",
    "\n",
    "total_elements = 0\n",
    "trainable_elements = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    numel = param.numel()\n",
    "    total_elements += numel\n",
    "    if param.requires_grad:\n",
    "        trainable_elements += numel\n",
    "        status = \"✅ trainable\"\n",
    "    else:\n",
    "        status = \"❌ frozen\"\n",
    "    print(f\"{status:12} | {name:60} | shape: {str(tuple(param.shape)):25} | #params: {numel}\")\n",
    "\n",
    "print(\"\\n📊 Summary\")\n",
    "print(f\"Trainable: {trainable_elements:,} / {total_elements:,} ({100 * trainable_elements / total_elements:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a102",
   "metadata": {},
   "source": [
    "# `get_peft_model()` でラップして実行\n",
    "\n",
    "ここまでで,  `ViTIntermediate` の実装をLoRA用にカスタマイズした際に，*LoRAを使用しない場合で，* 訓練・推論時に問題なさそうなことがわかった．\n",
    "\n",
    "次は3072x3072の行列にLoRAを適用してどうなるかチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cb836f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6144 || all params: 199127908 || trainable%: 0.0030854539987433603\n",
      "Trainable params: 0.003085% of total params (6144 / 199127908)\n"
     ]
    }
   ],
   "source": [
    "# pretrained modelのロード\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    ViTExperiment.ViT_PATH,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ").to(device)\n",
    "\n",
    "    # LoRAの設定\n",
    "lora_cfg = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"vit.encoder.layer.11.intermediate.repair\"],  # 明示的に11層だけ指定\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.train()\n",
    "model.print_trainable_parameters()  # LoRA param ∼ 100 k だけ\n",
    "\n",
    "# 訓練可能なパラメータ数の割合\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params / total_params:.6%} of total params ({trainable_params} / {total_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a0440b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 パラメータ一覧（trainable / untrainable 含む）\n",
      "❌ frozen     | base_model.model.vit.embeddings.cls_token                    | shape: (1, 1, 768)               | #params: 768\n",
      "❌ frozen     | base_model.model.vit.embeddings.position_embeddings          | shape: (1, 197, 768)             | #params: 151296\n",
      "❌ frozen     | base_model.model.vit.embeddings.patch_embeddings.projection.weight | shape: (768, 3, 16, 16)          | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.embeddings.patch_embeddings.projection.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.0.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.1.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.2.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.3.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.4.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.5.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.6.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.7.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.8.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.output.dense.weight     | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.output.dense.bias       | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.layernorm_before.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.layernorm_after.weight  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.9.layernorm_after.bias    | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.output.dense.weight    | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.output.dense.bias      | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.layernorm_before.bias  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.layernorm_after.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.10.layernorm_after.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.attention.query.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.attention.query.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.attention.key.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.attention.key.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.attention.value.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.attention.value.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.output.dense.weight | shape: (768, 768)                | #params: 589824\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.attention.output.dense.bias | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.intermediate.dense.weight | shape: (3072, 768)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.intermediate.dense.bias | shape: (3072,)                   | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.intermediate.repair.weight | shape: (3072, 3072)              | #params: 9437184\n",
      "✅ trainable  | base_model.model.vit.encoder.layer.11.intermediate.repair.lora_A.default.weight | shape: (1, 3072)                 | #params: 3072\n",
      "✅ trainable  | base_model.model.vit.encoder.layer.11.intermediate.repair.lora_B.default.weight | shape: (3072, 1)                 | #params: 3072\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.output.dense.weight    | shape: (768, 3072)               | #params: 2359296\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.output.dense.bias      | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.layernorm_before.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.layernorm_before.bias  | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.layernorm_after.weight | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.encoder.layer.11.layernorm_after.bias   | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.layernorm.weight                        | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.vit.layernorm.bias                          | shape: (768,)                    | #params: 768\n",
      "❌ frozen     | base_model.model.classifier.weight                           | shape: (100, 768)                | #params: 76800\n",
      "❌ frozen     | base_model.model.classifier.bias                             | shape: (100,)                    | #params: 100\n",
      "\n",
      "📊 Summary\n",
      "Trainable: 6,144 / 199,127,908 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"📋 パラメータ一覧（trainable / untrainable 含む）\")\n",
    "\n",
    "total_elements = 0\n",
    "trainable_elements = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    numel = param.numel()\n",
    "    total_elements += numel\n",
    "    if param.requires_grad:\n",
    "        trainable_elements += numel\n",
    "        status = \"✅ trainable\"\n",
    "    else:\n",
    "        status = \"❌ frozen\"\n",
    "    print(f\"{status:12} | {name:60} | shape: {str(tuple(param.shape)):25} | #params: {numel}\")\n",
    "\n",
    "print(\"\\n📊 Summary\")\n",
    "print(f\"Trainable: {trainable_elements:,} / {total_elements:,} ({100 * trainable_elements / total_elements:.2f}%)\")\n",
    "# 上の出力から，最終レイヤの中間状態のアップデートをLoRAしたいことがわかる\n",
    "# 訓練する重みパラメータの数 = 3072 x r x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "296e3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False, # img列がないとエラーになるので必要\n",
    "    evaluation_strategy=\"epoch\", # エポックの終わりごとにeval_datasetで評価\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"error\",\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61379c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel\n",
      "self.args.label_names=None\n",
      "default_label_names=['labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/transformers-4.30.2/src/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.610400</td>\n",
       "      <td>0</td>\n",
       "      <td>{'accuracy': 0.0}</td>\n",
       "      <td>{'f1': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.585800</td>\n",
       "      <td>0</td>\n",
       "      <td>{'accuracy': 0.0}</td>\n",
       "      <td>{'f1': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.629871845245361, logits=tensor([[-0.0772,  0.0243,  0.0111,  ...,  0.0638, -0.0674,  0.0436],\n",
      "        [-0.0770,  0.0245,  0.0113,  ...,  0.0636, -0.0673,  0.0437],\n",
      "        [-0.0770,  0.0244,  0.0112,  ...,  0.0636, -0.0673,  0.0436],\n",
      "        ...,\n",
      "        [-0.0769,  0.0242,  0.0111,  ...,  0.0636, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0242,  0.0112,  ...,  0.0636, -0.0673,  0.0434],\n",
      "        [-0.0770,  0.0243,  0.0112,  ...,  0.0635, -0.0672,  0.0437]],\n",
      "       device='cuda:0'), labels=tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,\n",
      "        70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.601065158843994, logits=tensor([[-0.0770,  0.0243,  0.0111,  ...,  0.0636, -0.0673,  0.0435],\n",
      "        [-0.0771,  0.0243,  0.0113,  ...,  0.0636, -0.0671,  0.0435],\n",
      "        [-0.0770,  0.0244,  0.0112,  ...,  0.0635, -0.0673,  0.0437],\n",
      "        ...,\n",
      "        [-0.0771,  0.0245,  0.0113,  ...,  0.0634, -0.0671,  0.0439],\n",
      "        [-0.0770,  0.0244,  0.0112,  ...,  0.0636, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0245,  0.0113,  ...,  0.0635, -0.0672,  0.0438]],\n",
      "       device='cuda:0'), labels=tensor([41, 93, 56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,\n",
      "         4,  6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.631938457489014, logits=tensor([[-0.0769,  0.0243,  0.0112,  ...,  0.0633, -0.0670,  0.0435],\n",
      "        [-0.0770,  0.0243,  0.0112,  ...,  0.0635, -0.0671,  0.0436],\n",
      "        [-0.0771,  0.0245,  0.0113,  ...,  0.0637, -0.0672,  0.0437],\n",
      "        ...,\n",
      "        [-0.0771,  0.0244,  0.0113,  ...,  0.0636, -0.0671,  0.0437],\n",
      "        [-0.0771,  0.0244,  0.0113,  ...,  0.0635, -0.0672,  0.0436],\n",
      "        [-0.0771,  0.0244,  0.0112,  ...,  0.0636, -0.0673,  0.0436]],\n",
      "       device='cuda:0'), labels=tensor([ 7, 35, 43, 82, 63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78,\n",
      "        54,  6, 29, 89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.619655609130859, logits=tensor([[-0.0771,  0.0242,  0.0112,  0.0862,  0.0574, -0.0195, -0.0603, -0.0901,\n",
      "         -0.0310, -0.0922,  0.0717,  0.1744, -0.0425, -0.0829, -0.0732, -0.0217,\n",
      "         -0.0563, -0.1417,  0.0185,  0.0385,  0.0250, -0.1234, -0.0135, -0.0338,\n",
      "          0.0841, -0.1059, -0.0090,  0.0304, -0.0004,  0.0179,  0.0278,  0.0444,\n",
      "         -0.1208, -0.1249, -0.1217, -0.0987,  0.1252, -0.0318, -0.0365,  0.0878,\n",
      "         -0.0263, -0.0871,  0.0246,  0.0294,  0.0723, -0.1968,  0.1448, -0.0740,\n",
      "          0.1802,  0.0868, -0.0036, -0.1787,  0.1374, -0.0201, -0.1243, -0.0271,\n",
      "          0.0072,  0.0341,  0.1719,  0.1258, -0.0548, -0.0568,  0.0257,  0.0324,\n",
      "          0.1549,  0.0487,  0.0531,  0.0272,  0.0020, -0.0460, -0.0644, -0.0574,\n",
      "          0.0778, -0.0086,  0.0333,  0.1092, -0.0263,  0.1754,  0.0850,  0.0373,\n",
      "          0.0117,  0.0754, -0.0682, -0.1531,  0.2053,  0.0027,  0.0145, -0.0732,\n",
      "          0.1556,  0.0091, -0.1106,  0.0159, -0.0433, -0.0844,  0.1012,  0.1038,\n",
      "          0.1137,  0.0637, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0244,  0.0113,  0.0862,  0.0573, -0.0195, -0.0603, -0.0902,\n",
      "         -0.0313, -0.0925,  0.0717,  0.1742, -0.0426, -0.0830, -0.0733, -0.0218,\n",
      "         -0.0562, -0.1418,  0.0186,  0.0386,  0.0251, -0.1232, -0.0137, -0.0338,\n",
      "          0.0841, -0.1062, -0.0090,  0.0303, -0.0005,  0.0178,  0.0277,  0.0445,\n",
      "         -0.1206, -0.1249, -0.1216, -0.0987,  0.1251, -0.0317, -0.0365,  0.0879,\n",
      "         -0.0261, -0.0868,  0.0246,  0.0297,  0.0722, -0.1968,  0.1449, -0.0739,\n",
      "          0.1801,  0.0869, -0.0037, -0.1787,  0.1374, -0.0201, -0.1243, -0.0271,\n",
      "          0.0072,  0.0337,  0.1719,  0.1257, -0.0548, -0.0568,  0.0254,  0.0323,\n",
      "          0.1550,  0.0486,  0.0530,  0.0271,  0.0021, -0.0463, -0.0643, -0.0576,\n",
      "          0.0776, -0.0086,  0.0333,  0.1091, -0.0262,  0.1753,  0.0848,  0.0372,\n",
      "          0.0117,  0.0754, -0.0680, -0.1530,  0.2052,  0.0026,  0.0146, -0.0732,\n",
      "          0.1557,  0.0093, -0.1107,  0.0157, -0.0433, -0.0843,  0.1011,  0.1038,\n",
      "          0.1136,  0.0636, -0.0673,  0.0436],\n",
      "        [-0.0771,  0.0244,  0.0112,  0.0862,  0.0573, -0.0196, -0.0603, -0.0901,\n",
      "         -0.0311, -0.0924,  0.0717,  0.1744, -0.0426, -0.0830, -0.0733, -0.0218,\n",
      "         -0.0562, -0.1418,  0.0185,  0.0385,  0.0250, -0.1234, -0.0136, -0.0338,\n",
      "          0.0842, -0.1061, -0.0091,  0.0304, -0.0005,  0.0178,  0.0277,  0.0445,\n",
      "         -0.1207, -0.1250, -0.1217, -0.0987,  0.1252, -0.0318, -0.0365,  0.0878,\n",
      "         -0.0262, -0.0870,  0.0245,  0.0296,  0.0723, -0.1969,  0.1449, -0.0739,\n",
      "          0.1802,  0.0869, -0.0038, -0.1786,  0.1373, -0.0202, -0.1242, -0.0271,\n",
      "          0.0071,  0.0338,  0.1720,  0.1257, -0.0548, -0.0567,  0.0255,  0.0323,\n",
      "          0.1549,  0.0486,  0.0530,  0.0271,  0.0021, -0.0461, -0.0643, -0.0575,\n",
      "          0.0777, -0.0086,  0.0334,  0.1091, -0.0261,  0.1754,  0.0850,  0.0372,\n",
      "          0.0118,  0.0755, -0.0681, -0.1531,  0.2052,  0.0026,  0.0145, -0.0733,\n",
      "          0.1556,  0.0092, -0.1106,  0.0157, -0.0433, -0.0843,  0.1012,  0.1038,\n",
      "          0.1136,  0.0637, -0.0673,  0.0436],\n",
      "        [-0.0770,  0.0245,  0.0112,  0.0864,  0.0572, -0.0197, -0.0605, -0.0902,\n",
      "         -0.0315, -0.0927,  0.0718,  0.1742, -0.0426, -0.0831, -0.0732, -0.0219,\n",
      "         -0.0561, -0.1420,  0.0186,  0.0387,  0.0252, -0.1232, -0.0135, -0.0336,\n",
      "          0.0841, -0.1064, -0.0090,  0.0302, -0.0006,  0.0176,  0.0277,  0.0445,\n",
      "         -0.1206, -0.1250, -0.1216, -0.0985,  0.1250, -0.0317, -0.0363,  0.0880,\n",
      "         -0.0261, -0.0868,  0.0246,  0.0299,  0.0724, -0.1969,  0.1449, -0.0739,\n",
      "          0.1802,  0.0869, -0.0040, -0.1786,  0.1372, -0.0200, -0.1243, -0.0271,\n",
      "          0.0072,  0.0335,  0.1721,  0.1257, -0.0549, -0.0568,  0.0252,  0.0320,\n",
      "          0.1550,  0.0486,  0.0529,  0.0270,  0.0021, -0.0464, -0.0642, -0.0577,\n",
      "          0.0775, -0.0087,  0.0335,  0.1091, -0.0261,  0.1753,  0.0848,  0.0371,\n",
      "          0.0117,  0.0754, -0.0679, -0.1532,  0.2052,  0.0027,  0.0145, -0.0732,\n",
      "          0.1556,  0.0093, -0.1106,  0.0155, -0.0433, -0.0842,  0.1012,  0.1039,\n",
      "          0.1136,  0.0635, -0.0672,  0.0436]], device='cuda:0'), labels=tensor([50, 75, 37, 35], device='cuda:0')\n",
      "output=EvalLoopOutput(predictions=array([[-0.07715344,  0.02430003,  0.01113093, ...,  0.06376767,\n",
      "        -0.06741786,  0.04355011],\n",
      "       [-0.07704495,  0.02446265,  0.01126758, ...,  0.06355281,\n",
      "        -0.06730512,  0.04368785],\n",
      "       [-0.07704444,  0.02435635,  0.0111902 , ...,  0.06357204,\n",
      "        -0.067281  ,  0.04360157],\n",
      "       ...,\n",
      "       [-0.07701375,  0.02439619,  0.01125483, ...,  0.06364172,\n",
      "        -0.06731378,  0.04356956],\n",
      "       [-0.07711188,  0.02443117,  0.01122118, ...,  0.06371067,\n",
      "        -0.06730203,  0.04364412],\n",
      "       [-0.07695779,  0.02447832,  0.01121109, ...,  0.06351195,\n",
      "        -0.06718923,  0.04360244]], dtype=float32), label_ids=array([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92,\n",
      "       97, 70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61, 41, 93,\n",
      "       56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,  4,\n",
      "        6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65,  7, 35, 43, 82,\n",
      "       63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78, 54,  6, 29,\n",
      "       89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21, 50, 75, 37, 35]), metrics={'eval_loss': 0, 'eval_accuracy': {'accuracy': 0.0}, 'eval_f1': {'f1': 0.0}}, num_samples=100)\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.62986946105957, logits=tensor([[-0.0772,  0.0243,  0.0111,  ...,  0.0638, -0.0674,  0.0436],\n",
      "        [-0.0771,  0.0245,  0.0113,  ...,  0.0636, -0.0673,  0.0437],\n",
      "        [-0.0771,  0.0244,  0.0112,  ...,  0.0636, -0.0673,  0.0436],\n",
      "        ...,\n",
      "        [-0.0769,  0.0242,  0.0111,  ...,  0.0636, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0242,  0.0112,  ...,  0.0636, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0243,  0.0112,  ...,  0.0635, -0.0672,  0.0437]],\n",
      "       device='cuda:0'), labels=tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,\n",
      "        70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.601061820983887, logits=tensor([[-0.0770,  0.0243,  0.0111,  ...,  0.0637, -0.0673,  0.0435],\n",
      "        [-0.0771,  0.0243,  0.0113,  ...,  0.0636, -0.0671,  0.0435],\n",
      "        [-0.0770,  0.0244,  0.0112,  ...,  0.0635, -0.0673,  0.0437],\n",
      "        ...,\n",
      "        [-0.0771,  0.0245,  0.0113,  ...,  0.0634, -0.0671,  0.0439],\n",
      "        [-0.0770,  0.0244,  0.0112,  ...,  0.0636, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0245,  0.0113,  ...,  0.0635, -0.0672,  0.0438]],\n",
      "       device='cuda:0'), labels=tensor([41, 93, 56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,\n",
      "         4,  6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.631938457489014, logits=tensor([[-0.0769,  0.0243,  0.0112,  ...,  0.0634, -0.0670,  0.0435],\n",
      "        [-0.0770,  0.0243,  0.0112,  ...,  0.0635, -0.0671,  0.0436],\n",
      "        [-0.0771,  0.0245,  0.0113,  ...,  0.0637, -0.0672,  0.0437],\n",
      "        ...,\n",
      "        [-0.0771,  0.0244,  0.0113,  ...,  0.0636, -0.0671,  0.0437],\n",
      "        [-0.0771,  0.0244,  0.0113,  ...,  0.0636, -0.0672,  0.0436],\n",
      "        [-0.0771,  0.0244,  0.0112,  ...,  0.0636, -0.0673,  0.0436]],\n",
      "       device='cuda:0'), labels=tensor([ 7, 35, 43, 82, 63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78,\n",
      "        54,  6, 29, 89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21],\n",
      "       device='cuda:0')\n",
      "self.label_names = ['labels']\n",
      "has_labels = True\n",
      "loss=4.619654178619385, logits=tensor([[-0.0771,  0.0242,  0.0112,  0.0862,  0.0574, -0.0195, -0.0603, -0.0901,\n",
      "         -0.0310, -0.0922,  0.0717,  0.1744, -0.0425, -0.0829, -0.0732, -0.0217,\n",
      "         -0.0564, -0.1417,  0.0185,  0.0385,  0.0250, -0.1234, -0.0135, -0.0338,\n",
      "          0.0841, -0.1059, -0.0090,  0.0304, -0.0004,  0.0179,  0.0278,  0.0444,\n",
      "         -0.1208, -0.1249, -0.1217, -0.0987,  0.1252, -0.0318, -0.0365,  0.0878,\n",
      "         -0.0263, -0.0871,  0.0246,  0.0294,  0.0722, -0.1968,  0.1448, -0.0740,\n",
      "          0.1802,  0.0868, -0.0036, -0.1787,  0.1374, -0.0201, -0.1243, -0.0271,\n",
      "          0.0072,  0.0341,  0.1719,  0.1258, -0.0548, -0.0568,  0.0257,  0.0324,\n",
      "          0.1549,  0.0487,  0.0531,  0.0272,  0.0020, -0.0460, -0.0644, -0.0574,\n",
      "          0.0778, -0.0086,  0.0333,  0.1092, -0.0263,  0.1754,  0.0850,  0.0373,\n",
      "          0.0117,  0.0754, -0.0682, -0.1531,  0.2053,  0.0027,  0.0145, -0.0732,\n",
      "          0.1556,  0.0092, -0.1106,  0.0159, -0.0433, -0.0844,  0.1012,  0.1038,\n",
      "          0.1137,  0.0637, -0.0673,  0.0435],\n",
      "        [-0.0770,  0.0244,  0.0113,  0.0862,  0.0573, -0.0195, -0.0603, -0.0902,\n",
      "         -0.0313, -0.0925,  0.0717,  0.1742, -0.0426, -0.0830, -0.0733, -0.0218,\n",
      "         -0.0562, -0.1418,  0.0186,  0.0386,  0.0251, -0.1232, -0.0137, -0.0337,\n",
      "          0.0841, -0.1062, -0.0090,  0.0303, -0.0005,  0.0178,  0.0277,  0.0445,\n",
      "         -0.1207, -0.1249, -0.1216, -0.0987,  0.1251, -0.0317, -0.0365,  0.0879,\n",
      "         -0.0261, -0.0869,  0.0246,  0.0297,  0.0722, -0.1968,  0.1448, -0.0739,\n",
      "          0.1801,  0.0869, -0.0037, -0.1787,  0.1373, -0.0202, -0.1243, -0.0271,\n",
      "          0.0072,  0.0337,  0.1719,  0.1257, -0.0549, -0.0568,  0.0254,  0.0323,\n",
      "          0.1550,  0.0486,  0.0530,  0.0271,  0.0021, -0.0462, -0.0643, -0.0576,\n",
      "          0.0776, -0.0086,  0.0333,  0.1091, -0.0262,  0.1753,  0.0848,  0.0372,\n",
      "          0.0117,  0.0754, -0.0680, -0.1530,  0.2052,  0.0026,  0.0146, -0.0732,\n",
      "          0.1557,  0.0093, -0.1107,  0.0157, -0.0433, -0.0843,  0.1011,  0.1038,\n",
      "          0.1136,  0.0637, -0.0673,  0.0436],\n",
      "        [-0.0771,  0.0244,  0.0112,  0.0862,  0.0573, -0.0196, -0.0603, -0.0901,\n",
      "         -0.0311, -0.0924,  0.0717,  0.1744, -0.0426, -0.0830, -0.0733, -0.0218,\n",
      "         -0.0562, -0.1418,  0.0186,  0.0385,  0.0250, -0.1234, -0.0136, -0.0338,\n",
      "          0.0842, -0.1061, -0.0091,  0.0304, -0.0005,  0.0178,  0.0277,  0.0445,\n",
      "         -0.1207, -0.1250, -0.1217, -0.0987,  0.1252, -0.0318, -0.0365,  0.0878,\n",
      "         -0.0262, -0.0870,  0.0246,  0.0296,  0.0723, -0.1969,  0.1449, -0.0739,\n",
      "          0.1802,  0.0869, -0.0038, -0.1786,  0.1373, -0.0202, -0.1242, -0.0271,\n",
      "          0.0071,  0.0338,  0.1720,  0.1257, -0.0548, -0.0567,  0.0255,  0.0323,\n",
      "          0.1549,  0.0486,  0.0530,  0.0271,  0.0021, -0.0461, -0.0642, -0.0575,\n",
      "          0.0777, -0.0086,  0.0334,  0.1091, -0.0261,  0.1754,  0.0850,  0.0372,\n",
      "          0.0118,  0.0755, -0.0681, -0.1531,  0.2052,  0.0026,  0.0145, -0.0733,\n",
      "          0.1556,  0.0092, -0.1106,  0.0157, -0.0433, -0.0843,  0.1012,  0.1038,\n",
      "          0.1136,  0.0637, -0.0673,  0.0436],\n",
      "        [-0.0770,  0.0245,  0.0112,  0.0864,  0.0572, -0.0197, -0.0605, -0.0902,\n",
      "         -0.0315, -0.0927,  0.0718,  0.1742, -0.0426, -0.0831, -0.0732, -0.0219,\n",
      "         -0.0561, -0.1420,  0.0186,  0.0387,  0.0252, -0.1232, -0.0135, -0.0336,\n",
      "          0.0841, -0.1064, -0.0090,  0.0302, -0.0006,  0.0176,  0.0277,  0.0445,\n",
      "         -0.1206, -0.1250, -0.1216, -0.0985,  0.1250, -0.0318, -0.0363,  0.0880,\n",
      "         -0.0261, -0.0868,  0.0246,  0.0299,  0.0724, -0.1969,  0.1449, -0.0739,\n",
      "          0.1802,  0.0869, -0.0039, -0.1786,  0.1372, -0.0200, -0.1243, -0.0271,\n",
      "          0.0072,  0.0335,  0.1721,  0.1257, -0.0549, -0.0568,  0.0252,  0.0320,\n",
      "          0.1550,  0.0486,  0.0529,  0.0270,  0.0021, -0.0464, -0.0642, -0.0577,\n",
      "          0.0775, -0.0087,  0.0335,  0.1091, -0.0261,  0.1753,  0.0848,  0.0371,\n",
      "          0.0117,  0.0754, -0.0679, -0.1532,  0.2052,  0.0027,  0.0145, -0.0732,\n",
      "          0.1556,  0.0093, -0.1106,  0.0155, -0.0433, -0.0842,  0.1012,  0.1039,\n",
      "          0.1136,  0.0635, -0.0672,  0.0436]], device='cuda:0'), labels=tensor([50, 75, 37, 35], device='cuda:0')\n",
      "output=EvalLoopOutput(predictions=array([[-0.07716382,  0.02430653,  0.01113006, ...,  0.06378087,\n",
      "        -0.06741952,  0.04355582],\n",
      "       [-0.07705535,  0.02446914,  0.01126672, ...,  0.06356603,\n",
      "        -0.0673068 ,  0.04369356],\n",
      "       [-0.07705481,  0.02436284,  0.01118933, ...,  0.06358527,\n",
      "        -0.06728266,  0.04360728],\n",
      "       ...,\n",
      "       [-0.07702413,  0.02440268,  0.01125398, ...,  0.06365494,\n",
      "        -0.06731546,  0.04357526],\n",
      "       [-0.07712226,  0.02443768,  0.01122031, ...,  0.06372389,\n",
      "        -0.0673037 ,  0.04364982],\n",
      "       [-0.07696819,  0.02448482,  0.01121024, ...,  0.06352518,\n",
      "        -0.06719092,  0.04360816]], dtype=float32), label_ids=array([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92,\n",
      "       97, 70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61, 41, 93,\n",
      "       56, 73, 58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,  4,\n",
      "        6,  9, 57,  2, 32, 71, 52, 42, 69, 77, 27, 15, 65,  7, 35, 43, 82,\n",
      "       63, 92, 66, 90, 67, 91, 32, 32, 82, 10, 77, 22, 71, 78, 54,  6, 29,\n",
      "       89, 78, 33, 11, 67, 22, 18, 27, 21, 13, 21, 50, 75, 37, 35]), metrics={'eval_loss': 0, 'eval_accuracy': {'accuracy': 0.0}, 'eval_f1': {'f1': 0.0}}, num_samples=100)\n"
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "# 訓練データを100件だけに制限\n",
    "ds_limited = ds_preprocessed.copy()\n",
    "ds_limited[\"train\"] = ds_preprocessed[\"train\"].select(range(100))\n",
    "ds_limited[\"test\"] = ds_preprocessed[\"test\"].select(range(100))\n",
    "# NOTE: 表示されるプログレスバーの分母の数字は，num_epoch*num_sample/batch_size\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds_preprocessed[\"train\"],\n",
    "    eval_dataset=ds_preprocessed[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120885c",
   "metadata": {},
   "source": [
    "パラメータ数は全然違う割にはあんまり訓練時間変わらなくない...？\n",
    "\n",
    "LoRAの論文でも変更するパラメータ数の割に精度が高いことを言ってるだけで，別に訓練時間の削減はいってなかったように見える．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
